---
title: "R Notebook"
output:
  html_document:
    df_print: paged
---

```{r warning=F, message=F} 
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(broom)

# Import Data Below
# read_csv()

PlacementData = read_csv('PlacementData.csv')
```

### Q2: What determines if individuals are placed?

We first perform a stepwise model selection through `stepAIC` function running on full dataset. 

```{r}
without.NA =  PlacementData %>% dplyr::select(-Salary, -WorkExp_1) %>% na.omit() %>% mutate(PlaceStatus = ifelse(JobStatus=="Placed", 1, 0)) %>% dplyr::select(-JobStatus)
full.model = glm(PlaceStatus~., data=without.NA, family="binomial")
best.model = MASS::stepAIC(full.model, trace=FALSE)
summary(best.model)
```

From the result above, we can see that the formula for prediction involves the following variables: 

- Gender
- SE_Grade
- HSE_Grade
- UG_Grade
- UG_Specialization
- WorkExp

And we perform a test to see its significance. 

```{r include=F}
G = best.model$null.deviance - best.model$deviance
Gdf = best.model$df.null - best.model$df.residual
1-pchisq(G, Gdf)
```

We also perform `bestglm` function onto the dataset and obtain sub-models. 

```{r}
rearranged.NA = without.NA %>% 
  dplyr::mutate(
    Gender=as.factor(Gender),
    SE_BoE=as.factor(SE_BoE),
    HSE_BoE=as.factor(HSE_BoE),
    HSE_Specialization=as.factor(HSE_Specialization),
    UG_Specialization=as.factor(UG_Specialization),
    WorkExp=as.factor(WorkExp),
    MBA_Specialization=as.factor(MBA_Specialization)
  ) %>% 
  fastDummies::dummy_cols(
    c("Gender", "SE_BoE", "HSE_BoE", "HSE_Specialization", "UG_Specialization", "WorkExp", "MBA_Specialization"),
    remove_first_dummy=TRUE) %>% 
  dplyr::select(where(is.numeric)) %>% 
  .[,c(1:5, 7:15, 6)] %>% 
  data.frame()
best.glms = bestglm::bestglm(rearranged.NA, family=binomial)
best.glms$BestModels
```



```{r}
glm.model1 = function(data) {
  return(glm(PlaceStatus~SE_Grade+HSE_Grade+UG_Grade+MBA_Grade+WorkExp, data=data, family="binomial"))
}
glm.model2 = function(data) {
  return(glm(PlaceStatus~SE_Grade+HSE_Grade+UG_Grade+MBA_Grade+Gender+UG_Specialization+WorkExp, data=data, family="binomial"))
}
glm.model3 = function(data) {
  return(glm(PlaceStatus~SE_Grade+HSE_Grade+UG_Grade+MBA_Grade+Gender+WorkExp, data=data, family="binomial"))
}
glm.model4 = function(data) {
  return(glm(PlaceStatus~SE_Grade+HSE_Grade+UG_Grade+MBA_Grade+UG_Specialization+WorkExp, data=data, family="binomial"))
}
glm.model5 = function(data) {
  return(glm(PlaceStatus~SE_Grade+HSE_Grade+UG_Grade+MBA_Grade+SE_BoE+WorkExp, data=data, family="binomial"))
}
stepaic.model = function(data) {
  return(glm(PlaceStatus~Gender+SE_Grade+HSE_Grade+UG_Grade+UG_Specialization+WorkExp+MBA_Grade, family="binomial", data=data))
}
```


```{r}
set.seed(100)
cv = without.NA %>% modelr::crossv_kfold(18)
pred = cv %>% dplyr::mutate(model1 = map(train, glm.model1),
                            model2 = map(train, glm.model2), 
                            model3 = map(train, glm.model3), 
                            model4 = map(train, glm.model4), 
                            model5 = map(train, glm.model5), 
                            model6 = map(train, stepaic.model))
pred.value = pred %>% 
  dplyr::mutate(predict1=map2(test, model1, ~augment(.y, newdata=.x, type.predict="response")), 
                predict2=map2(test, model2, ~augment(.y, newdata=.x, type.predict="response")),
                predict3=map2(test, model3, ~augment(.y, newdata=.x, type.predict="response")),
                predict4=map2(test, model4, ~augment(.y, newdata=.x, type.predict="response")),
                predict5=map2(test, model5, ~augment(.y, newdata=.x, type.predict="response")),
                predict6=map2(test, model6, ~augment(.y, newdata=.x, type.predict="response")),) 

model1 = pred.value %>% 
  dplyr::select(predict1) %>% 
  unnest() %>%
  mutate(
    predicted = ifelse(.fitted >= 0.5, "will Place", "wont Place"), 
    placed = ifelse(PlaceStatus > 0.5, "Placed", "Not Placed")) 

model2 = pred.value %>% 
  dplyr::select(predict2) %>% 
  unnest() %>% 
  mutate(
    predicted = ifelse(.fitted >= 0.5, "will Place", "wont Place"), 
    placed = ifelse(PlaceStatus > 0.5, "Placed", "Not Placed")) 

model3 = pred.value %>% 
  dplyr::select(predict3) %>% 
  unnest() %>% 
  mutate(
    predicted = ifelse(.fitted >= 0.5, "will Place", "wont Place"), 
    placed = ifelse(PlaceStatus > 0.5, "Placed", "Not Placed")) 

model4 = pred.value %>% 
  dplyr::select(predict4) %>% 
  unnest() %>% 
  mutate(
    predicted = ifelse(.fitted >= 0.5, "will Place", "wont Place"), 
    placed = ifelse(PlaceStatus > 0.5, "Placed", "Not Placed")) 

model5 = pred.value %>% 
  dplyr::select(predict5) %>% 
  unnest() %>% 
  mutate(
    predicted = ifelse(.fitted >= 0.5, "will Place", "wont Place"), 
    placed = ifelse(PlaceStatus > 0.5, "Placed", "Not Placed")) 

model6 = pred.value %>% 
  dplyr::select(predict6) %>% 
  unnest() %>% 
  mutate(
    predicted = ifelse(.fitted >= 0.5, "will Place", "wont Place"), 
         placed = ifelse(PlaceStatus > 0.5, "Placed", "Not Placed")) 

result1 = table(model1$placed, model1$predicted) %>% 
  prop.table() %>% 
  data.frame() %>% 
  transmute(Freq, status = paste(Var1, Var2, sep=":"))

result2 = table(model2$placed, model2$predicted) %>% 
  prop.table() %>% 
  data.frame() %>% 
  transmute(Freq, status = paste(Var1, Var2, sep=":"))

result3 = table(model3$placed, model3$predicted) %>% 
  prop.table() %>% 
  data.frame() %>% 
  transmute(Freq, status = paste(Var1, Var2, sep=":"))

result4 = table(model4$placed, model4$predicted) %>% 
  prop.table() %>% 
  data.frame() %>% 
  transmute(Freq, status = paste(Var1, Var2, sep=":"))

result5 = table(model5$placed, model5$predicted) %>%
  prop.table() %>%
  data.frame() %>%
  transmute(Freq, status = paste(Var1, Var2, sep=":"))

result6 = table(model6$placed, model6$predicted) %>%
  prop.table() %>% 
  data.frame() %>% 
  transmute(Freq, status = paste(Var1, Var2, sep=":"))

part1 = left_join(result1, result2, by="status", suffix=c("1", "2"))
part2 = left_join(result3, result4, by="status", suffix=c("3", "4"))
part3 = left_join(result5, result6, by="status", suffix=c("5", "6"))
final.result = left_join(left_join(part1, part2, by="status"), part3, by="status") %>% select(status, everything()) %>% rename(bestglm.mod1 = Freq1, bestglm.mod2 = Freq2, bestglm.mod3 = Freq3, bestglm.mod4 = Freq4, bestglm.mod5 = Freq5, stepAIC.mod = Freq6) 
# final.result
transposed.final = data.frame(t(final.result[, 2:7])) %>% dplyr::rename(n21=X1, n11=X2, n22=X3, n12=X4) %>% mutate(model=rownames(.)) %>% select(model, everything())
# transposed.final
final.table = transposed.final %>% mutate(Sensitivity = n11/(n11+n12), Specificity=n22/(n21+n22), fpr = n21/(n21+n22), fnr = n12/(n12+n11)) %>% select(-n21, -n22, -n11, -n12)

final.table
```

```{r}
mod4 = glm.model4(without.NA)
mod4
```

We can see that from the table, the best model that gives a lowest false positive rate is `bestglm.mod4`, with the following predictors: 

- SE_Grade
- HSE_Grade
- UG_Grade
- MBA_Grade
- UG_specialization
- WorkExp

and we apply this model to the overall dataset: 

```{r}

# Add plot here
last.model = glm.model4(without.NA)
prediction = without.NA
prediction$prediction = predict(last.model, prediction, type="response")
prediction = prediction %>% mutate(prediction = ifelse(prediction >= 0.5, 1, 0)) %>% mutate(correctness=ifelse(prediction == PlaceStatus, TRUE, FALSE))


ggplot(data=prediction, mapping=aes(x=prediction, y=PlaceStatus, color=correctness))+
  geom_jitter() +
  ggtitle("Distribution of predicted values vs actual values") + 
  xlab("Predicted Values") + 
  ylab("Actual Values")
```









